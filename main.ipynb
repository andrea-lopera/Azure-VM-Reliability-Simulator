{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0ec929b",
   "metadata": {},
   "source": [
    "# Azure VM Reliability Simulator\n",
    "\n",
    "## 1. Create synthetic data with LLM Scenarios\n",
    "## 2. Use Gradient Boosting to create a model with Time Series Split\n",
    "## 3. Survival Analysis with Time_Varying Covariates\n",
    "## 4. Streamlit Dashboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8868ed",
   "metadata": {},
   "source": [
    "### 1. Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f1d9a011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Initialize GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium') \n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium') \n",
    "\n",
    "# Generate failure scenario using GPT-2\n",
    "def generate_llm_failure_scenario():\n",
    "    \"\"\"Generate failure scenario using GPT-2\"\"\"\n",
    "    prompt = \"Azure VM failure scenario involving:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=100, do_sample=True)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Convert LLM description to telemetry signature\n",
    "def scenario_to_pattern(scenario: str):\n",
    "    \"\"\"Convert LLM description to telemetry signature\"\"\"\n",
    "    pattern = {\n",
    "        'cpu_util': np.random.normal(40, 5),\n",
    "        'mem_util': np.random.normal(50, 5),\n",
    "        'disk_io': np.random.exponential(80),\n",
    "        'net_latency': np.random.gamma(2, 10)\n",
    "    }\n",
    "    \n",
    "    # Pattern adjustments based on keywords\n",
    "    if 'CPU' in scenario or 'compute' in scenario:\n",
    "        pattern['cpu_util'] = min(100, pattern['cpu_util'] * 1.8)\n",
    "    if 'memory' in scenario or 'RAM' in scenario:\n",
    "        pattern['mem_util'] = min(100, pattern['mem_util'] * 1.7)\n",
    "    if 'disk' in scenario or 'storage' in scenario:\n",
    "        pattern['disk_io'] = pattern['disk_io'] * 3.5\n",
    "    if 'network' in scenario or 'latency' in scenario:\n",
    "        pattern['net_latency'] = pattern['net_latency'] * 4.0\n",
    "        \n",
    "    return pattern\n",
    "\n",
    "# Generate synthetic data\n",
    "def generate_azure_vm_data(num_vms=1000, days=90):\n",
    "    np.random.seed(42)\n",
    "    vm_ids = [f\"vm_{i:04d}\" for i in range(num_vms)]\n",
    "    dates = [datetime.now() - timedelta(days=x) for x in range(days)]\n",
    "    \n",
    "    # Pre-generate failure scenarios\n",
    "    failure_scenarios = [generate_llm_failure_scenario() for _ in range(20)]\n",
    "    failure_history = {vm: [] for vm in vm_ids}  # Track failure clusters\n",
    "\n",
    "    data = []\n",
    "    for vm_id in vm_ids:\n",
    "        sys_failures = 0\n",
    "        for i, date in enumerate(dates):\n",
    "            # Apply failure history decay (temporal dependency)\n",
    "            cluster_effect = 0.8 ** sys_failures\n",
    "\n",
    "            is_failure = False\n",
    "            if np.random.random() < (0.08 * cluster_effect):  # Failure event\n",
    "                scenario = np.random.choice(failure_scenarios)\n",
    "                pattern = scenario_to_pattern(scenario)\n",
    "                sys_failures += 1\n",
    "                is_failure = True\n",
    "                failure_history[vm_id].append((date, scenario))\n",
    "            else:\n",
    "                # Baseline with failure history influence\n",
    "                stress_factor = 1 + (0.3 * cluster_effect)\n",
    "                pattern = {\n",
    "                    'cpu_util': min(100, np.random.normal(40, 10) * stress_factor),\n",
    "                    'mem_util': min(100, np.random.normal(50, 10) * stress_factor),\n",
    "                    'disk_io': np.random.exponential(100) * stress_factor,\n",
    "                    'net_latency': np.random.gamma(2, 15) * stress_factor\n",
    "                }\n",
    "                \n",
    "            data.append({\n",
    "                \"vm_id\": vm_id,\n",
    "                \"timestamp\": date,\n",
    "                \"sys_failures\": 1 if is_failure else 0,\n",
    "                \"cumulative_failures\" : sys_failures,\n",
    "                **pattern\n",
    "            })\n",
    "            \n",
    "            # Decay failure counter weekly\n",
    "            if i % 7 == 0:\n",
    "                sys_failures = max(0, sys_failures - 1)\n",
    "                \n",
    "    return pd.DataFrame(data), failure_history\n",
    "\n",
    "# Generate data\n",
    "df, failure_history = generate_azure_vm_data()\n",
    "df.to_csv(\"azure_vm_telemetry.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7344abfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with 'sys_failures' == 0: 83354\n",
      "Number of rows with 'sys_failures' == 1:6646\n"
     ]
    }
   ],
   "source": [
    "count_of_zeros = (df['sys_failures'] == 0).sum()\n",
    "print(f\"Number of rows with 'sys_failures' == 0: {count_of_zeros}\")\n",
    "\n",
    "count_of_ones =(df['sys_failures'] == 1).sum()\n",
    "print(f\"Number of rows with 'sys_failures' == 1:{count_of_ones}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444eb906",
   "metadata": {},
   "source": [
    "### 2. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "49ef9a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold Accuracy: 0.9998\n",
      "Fold Accuracy: 0.9998\n",
      "Fold Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Feature engineering\n",
    "df['hour'] = pd.to_datetime(df['timestamp']).dt.hour\n",
    "df['is_peak'] = df['hour'].between(8, 18).astype(int)\n",
    "df['cpu_mem_ratio'] = df['cpu_util'] / (df['mem_util'] + 1e-5)\n",
    "df['failure'] = ((df['cpu_util'] > 90) & (df['disk_io'] > 250)).astype(int)\n",
    "\n",
    "# Time-based split\n",
    "X = df[['cpu_util', 'mem_util', 'disk_io', 'net_latency', 'is_peak', 'cpu_mem_ratio', 'cumulative_failures']]\n",
    "y = df['failure']\n",
    "tss = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "for train_index, test_index in tss.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    model = XGBClassifier(n_estimators=200, max_depth=7, learning_rate=0.1)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    print(f\"Fold Accuracy: {acc:.4f}\")\n",
    "\n",
    "# SHAP explainer\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Visualize feature impacts \n",
    "# Simple beeswarm plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.plots.beeswarm(shap_values, show=False)\n",
    "plt.title(\"Feature Impact on Failure Probability\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_impact.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d6eb1247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waterfall plot for specific VM (e.g., first in test set)\n",
    "# Reset index to ensure we have access to the original data\n",
    "X_test = X_test.reset_index(drop=False)\n",
    "\n",
    "# Get VM ID for the first sample\n",
    "vm_id = df.loc[X_test.index[0], 'vm_id']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.plots.waterfall(shap_values[1], show=False)\n",
    "plt.title(f\"Prediction Breakdown for VM: {vm_id} \")\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_waterfall.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "306669c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP values type: <class 'shap._explanation.Explanation'>\n",
      "SHAP values shape: (22500, 7)\n",
      "SHAP values content: .values =\n",
      "array([[-0.7454769 , -0.25389832, -0.67461485, ...,  0.        ,\n",
      "        -1.6225246 ,  0.        ],\n",
      "       [-0.707166  , -0.4896794 , -0.7498154 , ...,  0.        ,\n",
      "        -1.3823855 ,  0.        ],\n",
      "       [-0.76117057,  0.37186083, -0.9826427 , ...,  0.        ,\n",
      "        -1.4498997 ,  0.        ],\n",
      "       ...,\n",
      "       [-0.7185102 , -0.28562978, -0.63948375, ...,  0.        ,\n",
      "        -1.4684348 ,  0.        ],\n",
      "       [-0.7717303 , -0.43873173,  0.2981865 , ...,  0.        ,\n",
      "        -1.5809938 ,  0.        ],\n",
      "       [-0.7274917 , -0.28141418, -0.76712686, ...,  0.        ,\n",
      "        -1.4749256 ,  0.        ]], dtype=float32)\n",
      "\n",
      ".base_values =\n",
      "array([-9.568583, -9.568583, -9.568583, ..., -9.568583, -9.568583,\n",
      "       -9.568583], dtype=float32)\n",
      "\n",
      ".data =\n",
      "array([[4.62074717e+01, 6.96752033e+01, 1.11418688e+02, ...,\n",
      "        1.00000000e+00, 6.63183785e-01, 1.00000000e+00],\n",
      "       [3.74578404e+01, 4.69118308e+01, 1.19775893e+02, ...,\n",
      "        1.00000000e+00, 7.98473047e-01, 1.00000000e+00],\n",
      "       [3.30716260e+01, 7.36011307e+01, 1.85380455e+02, ...,\n",
      "        1.00000000e+00, 4.49335780e-01, 0.00000000e+00],\n",
      "       ...,\n",
      "       [3.45811031e+01, 6.78805366e+01, 2.58747960e+02, ...,\n",
      "        1.00000000e+00, 5.09440551e-01, 0.00000000e+00],\n",
      "       [3.54252243e+01, 6.10068620e+01, 4.64933627e+02, ...,\n",
      "        1.00000000e+00, 5.80675966e-01, 0.00000000e+00],\n",
      "       [4.28345625e+01, 6.37416020e+01, 2.39120450e+01, ...,\n",
      "        1.00000000e+00, 6.72003125e-01, 0.00000000e+00]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"SHAP values type: {type(shap_values)}\")\n",
    "print(f\"SHAP values shape: {shap_values.shape}\")\n",
    "print(f\"SHAP values content: {shap_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb4623d",
   "metadata": {},
   "source": [
    "### 3. Survival Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8e41288e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  coef  exp(coef)  se(coef)  coef lower 95%  coef upper 95%  \\\n",
      "covariate                                                                     \n",
      "cpu_util      0.000403   1.000403  0.000816       -0.001197        0.002003   \n",
      "disk_io       0.000019   1.000019  0.000080       -0.000138        0.000176   \n",
      "sys_failures -0.000031   0.999969  0.040276       -0.078970        0.078908   \n",
      "\n",
      "              exp(coef) lower 95%  exp(coef) upper 95%  cmp to         z  \\\n",
      "covariate                                                                  \n",
      "cpu_util                 0.998804             1.002005     0.0  0.493388   \n",
      "disk_io                  0.999862             1.000176     0.0  0.237799   \n",
      "sys_failures             0.924068             1.082104     0.0 -0.000774   \n",
      "\n",
      "                     p  -log2(p)  \n",
      "covariate                         \n",
      "cpu_util      0.621739  0.685620  \n",
      "disk_io       0.812037  0.300383  \n",
      "sys_failures  0.999382  0.000892  \n"
     ]
    }
   ],
   "source": [
    "from lifelines import CoxTimeVaryingFitter\n",
    "\n",
    "# Prepare survival data\n",
    "df = df.sort_values(['vm_id', 'timestamp'])\n",
    "df['start'] = (df['timestamp'] - df['timestamp'].min()).dt.days\n",
    "df['end'] = df['start'] + 1  # Daily intervals\n",
    "df['event'] = df['failure']\n",
    "\n",
    "# CTV model\n",
    "ctv = CoxTimeVaryingFitter(penalizer=0.1)\n",
    "ctv.fit(df[['vm_id', 'start', 'end', 'event', 'cpu_util', 'disk_io', 'sys_failures']],\n",
    "        id_col='vm_id',\n",
    "        event_col='event',\n",
    "        start_col='start',\n",
    "        stop_col='end')\n",
    "\n",
    "print(ctv.summary)\n",
    "ctv.plot()\n",
    "plt.tight_layout()\n",
    "plt.savefig('survival_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5d951c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save artifacts for dashboard\n",
    "import joblib\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lifelines import CoxTimeVaryingFitter\n",
    "import pickle\n",
    "\n",
    "# 1. Save Model (XGBoost)\n",
    "joblib.dump(model, 'model.pkl')\n",
    "\n",
    "# 2. Save Survival Model\n",
    "with open('survival_model.pkl', 'wb') as file:\n",
    "    pickle.dump(ctv, file)\n",
    "\n",
    "# 3. Save SHAP Explainer\n",
    "# - For Tree-based models\n",
    "joblib.dump(explainer, 'shap_explainer.joblib')\n",
    "\n",
    "# - For non-tree models (alternative)\n",
    "with open('shap_explainer.pkl', 'wb') as f:\n",
    "    pickle.dump(explainer, f)\n",
    "\n",
    "# 4. Save Dataset\n",
    "df.to_csv('vm_telemetry.csv')\n",
    "\n",
    "# 5. Save Failure History\n",
    "np.save('failure_history.npy', failure_history, allow_pickle=True)\n",
    "# Alternative: joblib.dump(failure_history, 'failure_history.joblib')\n",
    "\n",
    "# 6. Save Feature Names (Critical!)\n",
    "with open('feature_names.txt', 'w') as f:\n",
    "    f.write(','.join(X_train.columns.tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".global-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
