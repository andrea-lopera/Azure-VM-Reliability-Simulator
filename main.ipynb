{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0ec929b",
   "metadata": {},
   "source": [
    "# Azure VM Reliability Simulator\n",
    "\n",
    "## 1. Create synthetic data with LLM Scenarios\n",
    "## 2. Use Gradient Boosting to create a model with Time Series Split\n",
    "## 3. Survival Analysis with Time_Varying Covariates\n",
    "## 4. Streamlit Dashboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8868ed",
   "metadata": {},
   "source": [
    "### 1. Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1d9a011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Initialize GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium') \n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium') \n",
    "\n",
    "# Generate failure scenario using GPT-2\n",
    "def generate_llm_failure_scenario():\n",
    "    \"\"\"Generate failure scenario using GPT-2\"\"\"\n",
    "    prompt = \"Azure VM failure scenario involving:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=100, do_sample=True)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Convert LLM description to telemetry signature\n",
    "def scenario_to_pattern(scenario: str):\n",
    "    \"\"\"Convert LLM description to telemetry signature\"\"\"\n",
    "    pattern = {\n",
    "        'cpu_util': np.random.normal(40, 5),\n",
    "        'mem_util': np.random.normal(50, 5),\n",
    "        'disk_io': np.random.exponential(80),\n",
    "        'net_latency': np.random.gamma(2, 10)\n",
    "    }\n",
    "    \n",
    "    # Pattern adjustments based on keywords\n",
    "    if 'CPU' in scenario or 'compute' in scenario:\n",
    "        pattern['cpu_util'] = min(100, pattern['cpu_util'] * 1.8)\n",
    "    if 'memory' in scenario or 'RAM' in scenario:\n",
    "        pattern['mem_util'] = min(100, pattern['mem_util'] * 1.7)\n",
    "    if 'disk' in scenario or 'storage' in scenario:\n",
    "        pattern['disk_io'] = pattern['disk_io'] * 3.5\n",
    "    if 'network' in scenario or 'latency' in scenario:\n",
    "        pattern['net_latency'] = pattern['net_latency'] * 4.0\n",
    "        \n",
    "    return pattern\n",
    "\n",
    "# Generate synthetic data\n",
    "def generate_azure_vm_data(num_vms=1000, days=90):\n",
    "    np.random.seed(42)\n",
    "    vm_ids = [f\"vm_{i:04d}\" for i in range(num_vms)]\n",
    "    dates = [datetime.now() - timedelta(days=x) for x in range(days)]\n",
    "    \n",
    "    # Pre-generate failure scenarios\n",
    "    failure_scenarios = [generate_llm_failure_scenario() for _ in range(20)]\n",
    "    failure_history = {vm: [] for vm in vm_ids}  # Track failure clusters\n",
    "\n",
    "    data = []\n",
    "    for vm_id in vm_ids:\n",
    "        sys_failures = 0  # Failure counter per VM\n",
    "        for i, date in enumerate(dates):\n",
    "            # Apply failure history decay (temporal dependency)\n",
    "            cluster_effect = 0.8 ** sys_failures\n",
    "            \n",
    "            if np.random.random() < (0.0008 * cluster_effect):  # Failure event\n",
    "                scenario = np.random.choice(failure_scenarios)\n",
    "                pattern = scenario_to_pattern(scenario)\n",
    "                sys_failures += 1\n",
    "                failure_history[vm_id].append((date, scenario))\n",
    "            else:\n",
    "                # Baseline with failure history influence\n",
    "                stress_factor = 1 + (0.3 * cluster_effect)\n",
    "                pattern = {\n",
    "                    'cpu_util': min(100, np.random.normal(40, 10) * stress_factor),\n",
    "                    'mem_util': min(100, np.random.normal(50, 10) * stress_factor),\n",
    "                    'disk_io': np.random.exponential(100) * stress_factor,\n",
    "                    'net_latency': np.random.gamma(2, 15) * stress_factor\n",
    "                }\n",
    "                \n",
    "            data.append({\n",
    "                \"vm_id\": vm_id,\n",
    "                \"timestamp\": date,\n",
    "                \"sys_failures\": sys_failures,\n",
    "                **pattern\n",
    "            })\n",
    "            \n",
    "            # Decay failure counter weekly\n",
    "            if i % 7 == 0:\n",
    "                sys_failures = max(0, sys_failures - 1)\n",
    "                \n",
    "    return pd.DataFrame(data), failure_history\n",
    "\n",
    "# Generate data\n",
    "df, failure_history = generate_azure_vm_data()\n",
    "df.to_csv(\"azure_vm_telemetry_enhanced.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444eb906",
   "metadata": {},
   "source": [
    "### 2. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ef9a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold Accuracy: 0.9998\n",
      "Fold Accuracy: 0.9996\n",
      "Fold Accuracy: 0.9997\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Feature engineering\n",
    "df['hour'] = pd.to_datetime(df['timestamp']).dt.hour\n",
    "df['is_peak'] = df['hour'].between(8, 18).astype(int)\n",
    "df['cpu_mem_ratio'] = df['cpu_util'] / (df['mem_util'] + 1e-5)\n",
    "df['failure'] = ((df['cpu_util'] > 90) & (df['disk_io'] > 250)).astype(int)\n",
    "\n",
    "# Time-based split\n",
    "X = df[['cpu_util', 'mem_util', 'disk_io', 'net_latency', 'is_peak', 'cpu_mem_ratio', 'sys_failures']]\n",
    "y = df['failure']\n",
    "tss = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "for train_index, test_index in tss.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    model = XGBClassifier(n_estimators=200, max_depth=7, learning_rate=0.1)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    print(f\"Fold Accuracy: {acc:.4f}\")\n",
    "\n",
    "# SHAP explainer\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Visualize feature impacts \n",
    "# Simple beeswarm plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.plots.beeswarm(shap_values, show=False)\n",
    "plt.title(\"Feature Impact on Failure Probability\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_impact.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6eb1247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waterfall plot for specific VM (e.g., first in test set)\n",
    "# Reset index to ensure we have access to the original data\n",
    "X_test = X_test.reset_index(drop=False)\n",
    "\n",
    "# Get VM ID for the first sample\n",
    "vm_id = df.loc[X_test.index[0], 'vm_id']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.plots.waterfall(shap_values[1], show=False)\n",
    "plt.title(f\"Prediction Breakdown for VM: {vm_id} \")\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_waterfall.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "306669c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP values type: <class 'shap._explanation.Explanation'>\n",
      "SHAP values shape: (22500, 7)\n",
      "SHAP values content: .values =\n",
      "array([[-0.9474661 , -0.827411  , -0.01655163, ...,  0.        ,\n",
      "        -1.6715472 ,  0.        ],\n",
      "       [-0.91999   , -0.8168504 , -0.93316865, ...,  0.        ,\n",
      "        -1.0839238 ,  0.        ],\n",
      "       [-0.8896795 , -0.7002088 , -0.9466994 , ...,  0.        ,\n",
      "        -1.5178065 ,  0.        ],\n",
      "       ...,\n",
      "       [-0.92956054, -0.26610658, -0.8776772 , ...,  0.        ,\n",
      "        -1.7956102 ,  0.        ],\n",
      "       [-0.9292432 , -0.2019916 , -0.91630465, ...,  0.        ,\n",
      "        -1.824682  ,  0.        ],\n",
      "       [-0.8832913 , -0.77498275, -0.94274163, ...,  0.        ,\n",
      "        -1.4533677 ,  0.        ]], dtype=float32)\n",
      "\n",
      ".base_values =\n",
      "array([-9.267478, -9.267478, -9.267478, ..., -9.267478, -9.267478,\n",
      "       -9.267478], dtype=float32)\n",
      "\n",
      ".data =\n",
      "array([[ 41.17106172,  59.39595677, 250.22362462, ...,   1.        ,\n",
      "          0.69316258,   0.        ],\n",
      "       [ 63.07190799,  62.22955299,  63.61864371, ...,   1.        ,\n",
      "          1.01353609,   0.        ],\n",
      "       [ 45.26578095,  61.99239658, 111.46120933, ...,   1.        ,\n",
      "          0.73018267,   0.        ],\n",
      "       ...,\n",
      "       [ 66.97582502,  67.67307384, 105.51588366, ...,   1.        ,\n",
      "          0.98969666,   0.        ],\n",
      "       [ 55.14972451,  70.35806398,  13.20848571, ...,   1.        ,\n",
      "          0.78384358,   0.        ],\n",
      "       [ 37.59094272,  56.00112339,  65.98828495, ...,   1.        ,\n",
      "          0.67125325,   0.        ]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"SHAP values type: {type(shap_values)}\")\n",
    "print(f\"SHAP values shape: {shap_values.shape}\")\n",
    "print(f\"SHAP values content: {shap_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb4623d",
   "metadata": {},
   "source": [
    "### 3. Survival Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e41288e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  coef  exp(coef)  se(coef)  coef lower 95%  coef upper 95%  \\\n",
      "covariate                                                                     \n",
      "cpu_util      0.000629   1.000630  0.000807       -0.000951        0.002210   \n",
      "disk_io       0.000036   1.000036  0.000081       -0.000123        0.000196   \n",
      "sys_failures -0.002308   0.997695  0.186746       -0.368324        0.363708   \n",
      "\n",
      "              exp(coef) lower 95%  exp(coef) upper 95%  cmp to         z  \\\n",
      "covariate                                                                  \n",
      "cpu_util                 0.999049             1.002213     0.0  0.780311   \n",
      "disk_io                  0.999877             1.000196     0.0  0.446204   \n",
      "sys_failures             0.691893             1.438654     0.0 -0.012360   \n",
      "\n",
      "                     p  -log2(p)  \n",
      "covariate                         \n",
      "cpu_util      0.435208  1.200223  \n",
      "disk_io       0.655450  0.609443  \n",
      "sys_failures  0.990139  0.014298  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AndreaLopera/.global-venv/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning:\n",
      "\n",
      "Column sys_failures have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['event'].astype(bool)\n",
      ">>> print(df.loc[events, 'sys_failures'].var())\n",
      ">>> print(df.loc[~events, 'sys_failures'].var())\n",
      "\n",
      "A very low variance means that the column sys_failures completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lifelines import CoxTimeVaryingFitter\n",
    "\n",
    "# Prepare survival data\n",
    "df = df.sort_values(['vm_id', 'timestamp'])\n",
    "df['start'] = (df['timestamp'] - df['timestamp'].min()).dt.days\n",
    "df['end'] = df['start'] + 1  # Daily intervals\n",
    "df['event'] = df['failure']\n",
    "\n",
    "# CTV model\n",
    "ctv = CoxTimeVaryingFitter(penalizer=0.1)\n",
    "ctv.fit(df[['vm_id', 'start', 'end', 'event', 'cpu_util', 'disk_io', 'sys_failures']],\n",
    "        id_col='vm_id',\n",
    "        event_col='event',\n",
    "        start_col='start',\n",
    "        stop_col='end')\n",
    "\n",
    "print(ctv.summary)\n",
    "ctv.plot()\n",
    "plt.tight_layout()\n",
    "plt.savefig('survival_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d951c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save artifacts for dashboard\n",
    "import joblib\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lifelines import CoxTimeVaryingFitter\n",
    "import pickle\n",
    "\n",
    "# 1. Save Model (XGBoost)\n",
    "joblib.dump(model, 'model.pkl')\n",
    "\n",
    "# 2. Save Survival Model\n",
    "with open('survival_model.pkl', 'wb') as file:\n",
    "    pickle.dump(ctv, file)\n",
    "\n",
    "# 3. Save SHAP Explainer\n",
    "# - For Tree-based models\n",
    "joblib.dump(explainer, 'shap_explainer.joblib')\n",
    "\n",
    "# - For non-tree models (alternative)\n",
    "with open('shap_explainer.pkl', 'wb') as f:\n",
    "    pickle.dump(explainer, f)\n",
    "\n",
    "# 4. Save Dataset\n",
    "df.to_csv('vm_telemetry.csv')\n",
    "\n",
    "# 5. Save Failure History\n",
    "np.save('failure_history.npy', failure_history, allow_pickle=True)\n",
    "# Alternative: joblib.dump(failure_history, 'failure_history.joblib')\n",
    "\n",
    "# 6. Save Feature Names (Critical!)\n",
    "with open('feature_names.txt', 'w') as f:\n",
    "    f.write(','.join(X_train.columns.tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".global-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
